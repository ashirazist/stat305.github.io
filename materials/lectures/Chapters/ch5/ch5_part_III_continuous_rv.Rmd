---

output: 
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false

---

```{r setup, echo=FALSE, message=FALSE}
library(knitr)
library(tidyverse)
library(xtable)
library(MASS)
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, fig.height = 2)
theme_set(theme_bw(base_family = "serif"))
set.seed(305)
```

```{r wrap-hook, echo=FALSE, include=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
```
class: center, middle, inverse
layout: yes
name: inverse

## STAT 305: Chapter 5 
### Part III
### Amin Shirazi
.footnote[Course page: [ashirazist.github.io/stat305.github.io](https://ashirazist.github.io/stat305.github.io/)]  
---
layout: true
class: center, middle, inverse
---
# Continuous Random Variables
## Terminology, Use, and Common Distributions
---
# What is a Continuous Random Variable?
---
layout:false
.left-column[
## Background
### What?
]
.right-column[
## Background on Continuous Random Variable

Along with discrete random variables, we have continuous random variables. While discrete random variables take one specific values from a _discrete_ (aka countable) set of possible real-number values, continous random variables take values over intervals of real numbers.

>**def: Continuous random variable ** </br>
>A continuous random variable is a random variable which takes values on a continuous interval of real numbers.

The reason we treat them differently has mainly to do with the differences in how the math behaves: now that we are dealing with interval ranges, we change summations to integrals.
]
---
layout:false
.left-column[
## Background
### What?
]
.right-column[


###Examples of continuous random variable:


> **Z** is the amount of torque required to lossen the next bold (not rounded)

> **T** is the time you will wait for the next bus

> **C** is the outside temprature at 11:49 pm tomorrow

> **L** is the length of the next manufactured metal bar

> **V** is the $%$ yield of the next run of process

]


---
layout: true
class: center, middle, inverse
---
# Terminology and Usage
---
layout:false
.left-column[
## Background
## Terminology
### pdf
]
.right-column[
### Probability Density Function

Since we are now taking values over an interval, we can not "add up" probabilities with our probability function anymore. Instead, we need a new function to describe probability:

>**def: probability density function** </br>
>A probability density function (pdf) defines the way the probability of a continuous random variable is distributed across the interval of values it can take. Since it represents probability, the probability function must always be non-negative. Regions of higher density have higher probability.

]
---

layout:false
.left-column[
## Background
## Terminology
### pdf
]
.right-column[
### Probability Density Function
####Validity of a *pdf*
Any function that satisfies the following can be a probability density function:

1. $\int_{-\infty}^{\infty} f(x) dx = 1$

2. $f(x) \ge 0$ for all $x$ in $(-\infty, \infty)$


 and such that for all $a \le b$,
    $$P(a \le X \le b) = P(a \le X < b) =\\ P(a < X \le b) = P(a < X < b)\\ =\int\limits_a^bf(x)dx.$$

]
---
layout:false
.left-column[
## Background
## Terms and Use
### pdf
]
.right-column[
### Probability Density Function

With continuous random variables, we use pdfs to get probabilities as follows:

>For a continuous random variable $X$ with probability density function $f(x)$, 
>$$P(a \le X \le b) = \int_{a}^{b} f(x) dx$$

>for any real values $a, b$ such that $a\le b$

<!-- **Drawing: Figure 5.6 in book** -->

```{r, fig.height=3}
x <- seq(-5, 22, length.out = 200)
f <- .6*dnorm(x, 3, 2) + .3*dnorm(x, 9, 1) + .1*dnorm(x, 19, .4)
shade <- rbind(data.frame(x = x[x >= 2 & x <= 6], y = f[x >= 2 & x <= 6]),
               data.frame(x = rev(x[x >= 2 & x <= 6]), y = 0))
qplot(x, f, geom = "line") +
  geom_polygon(aes(x, y), data = shade, fill = "blue", alpha = .4) +
  ylab(expression("f(x)"))
```


]
---
layout:false
.left-column[
## Background
## Terms and Use
### pdf
]
.right-column[

###Example

Consider a de-magnetized compass needle mounted at its center so that it can spin freely.  It is spun clockwise and when it comes to rest the angle, $\theta$, from the vertical, is measured. Let 
$$Y = \text{the angle measured after each spin in radians}$$
What values can $Y$ take?

&nbsp;

What form makes sense for $f(y)$?

]

---
layout:false
.left-column[
## Background
## Terms and Use
### pdf
]
.right-column[

###Example

If this form is adopted, that what must the pdf be?

&nbsp;

&nbsp;

&nbsp;

&nbsp;

Using this pdf, calculate the following probabilities:
- $P[Y < \frac{\pi}{2}]$


]
---
layout:false
.left-column[
## Background
## Terms and Use
### pdf
]
.right-column[

###Example


- $P[\frac{\pi}{2} < Y < 2\pi]$

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;


- $P[Y = \frac{\pi}{6}]$
]
---
layout:false
.left-column[
## Background
## Terms and Use
### pdf
### cdf
]
.right-column[
### Cumulative Density Function (**CDF**)

We also have the cumulative density function for continuous random variables:
>**def: Cumulative density function (cdf)**
>For a continous random variable, $X$, with pdf f(x) the cumulative density function $F(x)$ is defined as the probability that $X$ takes a value less than or equal to $x$ which is to say
>$$ F(x) = P(X \le x) = \int_{-\infty}^{x} f(t) dt $$

TRUE FACT: the Fundamental Theorem of Calculus applies here:
$$ \dfrac{d}{dx} F(x) = f(x) $$

]
---
layout:false
.left-column[
## Background
## Terms and Use
### pdf
### cdf
]
.right-column[
### Cumulative Density Function (**CDF**)
#### Properties of CDF for continuous random variables


As with discrete random variables, $F$ has the following properties:


- **F** is monotonically increasing (i.e it is never decreasing)

- $\lim_{x\rightarrow-\infty}{F(x)}= 0$ and $\lim_{x\rightarrow+\infty}{F(x)}= 1$ 

    - This means that $0\leq{F(x)}\le 1$ for **any CDF**

- **F** is *continuous*. (instead of just right continuous in discrete form)

]
---
layout: true
class: center, middle, inverse
---
##Mean and Variance 
###of
##Continuous Random Variables
---
layout:false
.left-column[
## Background
## Terms and Use
### pdf
### cdf
### E(X), V(X)
]
.right-column[
### Expected Value and Variance

#### Expected Value
As with discrete random variables, continuous random variables have expected values and variances:
>**def: Expected Value of Continuous Random Variable** </br>
>For a continous random variable, $X$, with pdf f(x) the expected value (also known as the mean) is defined as
>$$ E(X) = \int_{-\infty}^{\infty} x f(x) dx $$

We often use the symbol $\mu$ for the mean of a random variable, since writing $E(X)$ can get confusing when lots of other parenthesis are around. We also sometimes write $EX$.
]
---
layout:false
.left-column[
## Background
## Terms and Use
### pdf
### cdf
### E(X), V(X)
]
.right-column[
### Expected Value and Variance

#### Variance
>**def: Variance of Continuous Random Variable** </br>
>For a continous random variable, $X$, with pdf f(x) and expected value $\mu$, the variance is defined as
>$$ V(X) = \int_{-\infty}^{\infty} (x - \mu)^2 \cdot f(x) dx $$
>which is identical to saying
>$$ V(X) = E(X^2) - E(X)^2 $$

We will sometimes use the symbol $\sigma^2$ to refer to the variance and you may see the notation $Var X$ or $VX$ as well.
]
---
layout:false
.left-column[
## Background
## Terms and Use
### pdf
### cdf
### E(X), V(X)
]
.right-column[
### Expected Value and Variance

#### Sdandard Deviation (SD)
We can also use the variance to get the standard deviation of the random variable:
>**def: Standard Deviation of Continuous Random Variable** </br>
>For a continous random variable, $X$, with pdf f(x) and expected value $\mu$, the standard deviation is defined as:
>$$ \sigma = \sqrt{\sigma^2} = \sqrt{\int_{-\infty}^{\infty} (x - \mu)^2 \cdot f(x) dx} $$

]
---
layout:false
.left-column[
## Background
## Terms and Use
### pdf
]
.right-column[
### Expected Value and Variance: Example
####Library books
Let $X$ denote the amount of time for which a book on $2$-hour hold reserve at a college library is checked out by a randomly selected student and suppose its density function is
$$
f(x) = \begin{cases}
0.5x & 0 \le x \le 2 \\\\
0 & \text{otherwise} 
\end{cases} $$

Calculate $\text{E}X$ and $\text{Var}X$.


]
---
layout: true
class: center, middle, inverse
---
##An important point about Expected Value
##and Variance of Random Variables
---
layout:false
.left-column[
## Background
## Terms and Use
### pdf
]
.right-column[
### Expected Value and Variance:

For a linear function, $g(X) = aX + b$, where $a$ and $b$ are constants,

> $\text{E}(aX + b)= a \text{E}(X) + b$
 
> $\text{Var}(aX + b)= a^2 \text{Var}(X)$

e.g Let $X\sim Binomial(5, 0.2)$. What is the expected value and variance of 4X- 3?


]
---
layout: true
class: center, middle, inverse
---
# Common Distributions
## Uniform Distribution
---
layout:false
.left-column[
## Background
## Terms and Use
## Common Dists
### Uniform
]
.right-column[

## Common Distributions

### Uniform Distribution

For cases where we only know/believe/assume that a value will be between two numbers but know/believe/assume _nothing_ else.

**Origin**: We know a the random variable will take a value inside a certain range, but we don't have any belief that one part of that range is more likely than another part of that range.

>**Definition: Uniform random variable **</br>
>The random variable $U$ is a uniform random variable on the interval $[a, b]$ if it's density is constant on $[a, b]$ and the probability it takes a value outside $[a, b]$ is 0. We say that $U$ follows a uniform distribution or $U \sim uniform(a, b)$.

]
---
layout:false
.left-column[
## Background
## Terms and Use
## Common Dists
### Uniform
]
.right-column[
### Uniform Distribution

>**Definition: Uniform pdf** </br>
>If $U$ is a uniform random variable on $[a, b]$ then the probability density function of $U$ is given by
>$$f(u) = \begin{cases}
> \dfrac{1}{b-a} & a \le u \le b \\\\
> 0 & o.w.
> \end{cases}
>$$

With this, we can find the for any value of $a$ and $b$, if $U \sim uniform(a, b)$ the mean and variance are:

$$
E(U) = \frac{1}{2}(b-a)
$$

$$
Var(U) = \frac{1}{12}(b-a)^2
$$

]
---
layout:false
.left-column[
## Background
## Terms and Use
## Common Dists
### Uniform
]
.right-column[
### Uniform Distribution

>**Definition: Uniform cdf** </br>
>If $U$ is a uniform random variable on $[a, b]$ then the cumulative density function of $U$ is given by
>$$F(u) = \begin{cases}
> 0 & u < a \\\\
> \dfrac{u-a}{b-a} & a \le u \le b \\\\
> 1 & u > b \\\\
>\end{cases}
>$$
]
---
layout:false
.left-column[
## Background
## Terms and Use
## Common Dists
### Uniform
]
.right-column[
### Uniform Distribution

A few useful notes:

- The most commonly used uniform random variable is $U \sim Uniform(0,1)$.

- Again, this is useful if we want to use a random variable that takes values within an interval, but we don't think it is likely to be in any certain region. 

- The values $a$ and $b$ used to determine the range in which $f(u)$ is not 0 are parameters of the distribution.
]
---
layout:true
class: middle, center, inverse
---
# Common Distributions
##Exponential Distribution

---
layout:false
.left-column[
## Background
## Terms and Use
## Common Dists
### Uniform
### Exponential
]
.right-column[

### Exponential Distribution

**Origin**: Often used in to take the values of waiting times until some specific occurance where the idea of "memorylessness" is important - meaning that if you expect to see the occurance in 1 minute after waiting 5 minutes you still expect to see the occurance in 1 minute.

>**Definition: Exponential random variable **</br>
>The random variable $X$ is a exponential random variable on the interval $[0, \infty]$ if it's density drops exponentially with rate $\frac{1}{\alpha}$ as you move away from 0. We say that $X$ follows a exponential distribution or $X \sim exponential(\alpha)$.
]
---
layout:false
.left-column[
## Background
## Terms and Use
## Common Dists
### Uniform
### Exponential
]
.right-column[

### Exponential Distribution

>**Definition: Exponential pdf** </br>
>If $X$ is an exponential random variable with rate $\frac{1}{\alpha}$ then the probability density function of $X$ is given by
>$$
>f(u) = \begin{cases}
>\dfrac{1}{\alpha} e^{-\frac{x}{\alpha}} & x \ge 0 \\\\
>0 & o.w.
>\end{cases}
>$$

From this, we can derive:

$$
E(X) = \alpha
$$

$$
Var(X) = \alpha^2
$$

]
---
layout:false
.left-column[
## Background
## Terms and Use
## Common Dists
### Uniform
### Exponential
]
.right-column[
### Exponential Distribution

>**Definition: Exponential cdf** </br>
>If $X$ is a exponential random variable with rate $1/\alpha$ then the cumulative density function of $X$ is given by
>$$
>F(x) = \begin{cases}
>1 - exp(-x/\alpha) & 0 \le x \\\\
>0 & x < 0 \\\\
>\end{cases}
>$$

We sometimes write the exponential part as $exp(-x/\alpha)$ for clarity. 

]
---
layout:false
.left-column[
## Background
## Terms and Use
## Common Dists
### Uniform
### Exponential
]
.right-column[
### Exponential Distribution

**The memoryless property of exponential random variables**

Exponential random variables have what is called a memory less property. That is, that if we know the random variable is greater than $s$ then the probability that it is greater than $s + t$ can be found by the following:
$$
P(X > s + t | X > s) = P(X > t)
$$
In other words, if we knowing that a process has not occured _yet_ does not change the probability that it will occur _soon_. The waiting time is forgotten. This can be both good and bad, depending on what the event you are waiting for is.

**Example**: An elec. surge passes through a resistor randomly following an exponential distribution with a rate of once every hour. It has been two hours since the last surge. How long should we wait expect to wait for the next surge?

]
---
layout:false
.left-column[
## Background
## Terms and Use
## Common Dists
### Uniform
### Exponential
]
.right-column[
### Exponential Distribution

**Example**

Suppose that $X \sim exponential(.5)$. Find the probability that $X > \mu$.

]



